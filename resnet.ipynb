{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models, losses, Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('wclassesFiltered.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_p in range(df1.shape[0]):\n",
    "    img_path = df1.iloc[img_p, 1]\n",
    "    if \"Ã¢Â\\xa0\" in img_path:\n",
    "        img_path = img_path.replace('Ã¢Â', '\\u00E2')\n",
    "        temp_path = img_path.replace('\\u00E2', 'a') \n",
    "        df1.iat[img_p, 1] = temp_path\n",
    "    df1.iat[img_p, 4] = df1.iloc[img_p, 4]-140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[:,'style'] = df1['style'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df1.sample(frac=0.8,random_state=200)\n",
    "test = df1.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5120 validated image filenames belonging to 9 classes.\n",
      "Found 1280 validated image filenames belonging to 9 classes.\n",
      "Found 1600 validated image filenames belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "target_size=(256,256) # set the size of the images\n",
    "color_mode='rgb' # set the type of image\n",
    "class_mode= 'categorical' # set the class mode\n",
    "batch_size=64  # set the batch size\n",
    "subset='training' # set to 'training', or 'valiatiom' or leave as None\n",
    "\n",
    "train_gen = ImageDataGenerator(validation_split=0.2)\n",
    "test_gen = ImageDataGenerator()\n",
    "val_gen = ImageDataGenerator()\n",
    "\n",
    "train_imgs= train_gen.flow_from_dataframe(dataframe=train, \n",
    "          x_col='file',\n",
    "          y_col='style',\n",
    "          target_size=target_size,color_mode=color_mode,\n",
    "          class_mode=class_mode, batch_size=batch_size,shuffle=True, seed=123,\n",
    "          subset='training')\n",
    "val_imgs= train_gen.flow_from_dataframe(dataframe=train, \n",
    "          x_col='file',\n",
    "          y_col='style',\n",
    "          target_size=target_size,color_mode=color_mode,\n",
    "          class_mode=class_mode, batch_size=batch_size,shuffle=True, seed=123,\n",
    "          subset=\"validation\")\n",
    "test_imgs= test_gen.flow_from_dataframe(dataframe=test, \n",
    "          x_col='file',\n",
    "          y_col='style',\n",
    "          target_size=target_size,color_mode=color_mode,\n",
    "          class_mode=class_mode, batch_size=batch_size,shuffle=True, seed=123,\n",
    "          subset='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet é uma ANN (artificial neural network), variante da HigwayNet (https://en.wikipedia.org/wiki/Highway_network). Pode possuir centenas de camadas, sendo muito mais profunda do que redes neurais anteriores. São usados conexões de salto ou atalhos ​​para saltar sobre algumas camadas. Modelos ResNet típicos são implementados com saltos de camada dupla ou tripla que contêm não linearidades (ReLU) e normalização de lote no meio. Modelos com vários saltos paralelos são referidos como DenseNets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"File_ResNets.png\" style=\"height: 300px;\"/>   \n",
    "\n",
    "Forma canônica de uma rede neural residual. Uma camada ℓ   − 1 é ignorada na ativação de ℓ  − 2.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arquitetura ResNet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"arquitetura_resnet.png\" style=\"height: 350px;\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ResNet de 34 camadas com conexão Skip/Atalho (superior), rede simples de 34 camadas (meio), VGG-19 de 19 camadas (inferior)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O VGG-19 [2] (abaixo na imagem) é uma abordagem de última geração no ILSVRC 2014.\n",
    "A rede plana de 34 camadas (no meio) é tratada como a rede mais profunda do VGG-19 , ou seja, camadas mais convolucionais.\n",
    "A rede residual de 34 camadas (ResNet) (topo) é a simples com adição de conexão skip/atalho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ResNet usada é de grande profundidade, contendo **152** camadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.ResNet152(weights = 'imagenet', include_top = False, input_shape = (256,256,3))\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Flatten()(base_model.output)\n",
    "x = layers.Dense(1000, activation='relu')(x)\n",
    "predictions = layers.Dense(9, activation = 'softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model = Model(inputs = base_model.input, outputs = predictions)\n",
    "head_model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "80/80 [==============================] - 36s 350ms/step - loss: 32.9757 - accuracy: 0.6670 - val_loss: 2.4264 - val_accuracy: 0.7453\n",
      "Epoch 2/40\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 0.4822 - accuracy: 0.9131 - val_loss: 1.3212 - val_accuracy: 0.8281\n",
      "Epoch 3/40\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 0.1040 - accuracy: 0.9787 - val_loss: 1.3569 - val_accuracy: 0.8062\n",
      "Epoch 4/40\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 0.0468 - accuracy: 0.9936 - val_loss: 1.3235 - val_accuracy: 0.8375\n",
      "Epoch 5/40\n",
      "80/80 [==============================] - 25s 315ms/step - loss: 0.0563 - accuracy: 0.9957 - val_loss: 1.3732 - val_accuracy: 0.8359\n",
      "Epoch 6/40\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 0.0459 - accuracy: 0.9969 - val_loss: 1.3113 - val_accuracy: 0.8359\n",
      "Epoch 7/40\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 0.0355 - accuracy: 0.9965 - val_loss: 1.3489 - val_accuracy: 0.8352\n",
      "Epoch 8/40\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 0.0466 - accuracy: 0.9971 - val_loss: 1.3433 - val_accuracy: 0.8289\n",
      "Epoch 9/40\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 0.0234 - accuracy: 0.9975 - val_loss: 1.3032 - val_accuracy: 0.8313\n",
      "Epoch 10/40\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 0.0321 - accuracy: 0.9971 - val_loss: 1.3588 - val_accuracy: 0.8258\n",
      "Epoch 11/40\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 0.0256 - accuracy: 0.9977 - val_loss: 1.3896 - val_accuracy: 0.8266\n",
      "Epoch 12/40\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 0.0225 - accuracy: 0.9979 - val_loss: 1.3338 - val_accuracy: 0.8234\n",
      "Epoch 13/40\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 0.0149 - accuracy: 0.9980 - val_loss: 1.3845 - val_accuracy: 0.8242\n",
      "Epoch 14/40\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 0.0116 - accuracy: 0.9979 - val_loss: 1.3429 - val_accuracy: 0.8281\n",
      "Epoch 15/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0151 - accuracy: 0.9975 - val_loss: 1.3090 - val_accuracy: 0.8352\n",
      "Epoch 16/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0086 - accuracy: 0.9982 - val_loss: 1.2619 - val_accuracy: 0.8359\n",
      "Epoch 17/40\n",
      "80/80 [==============================] - 26s 319ms/step - loss: 0.0067 - accuracy: 0.9982 - val_loss: 1.2622 - val_accuracy: 0.8359\n",
      "Epoch 18/40\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 0.0047 - accuracy: 0.9980 - val_loss: 1.3101 - val_accuracy: 0.8320\n",
      "Epoch 19/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0037 - accuracy: 0.9982 - val_loss: 1.2936 - val_accuracy: 0.8344\n",
      "Epoch 20/40\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 0.0031 - accuracy: 0.9977 - val_loss: 1.2825 - val_accuracy: 0.8328\n",
      "Epoch 21/40\n",
      "80/80 [==============================] - 25s 316ms/step - loss: 0.0028 - accuracy: 0.9975 - val_loss: 1.2706 - val_accuracy: 0.8383\n",
      "Epoch 22/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0030 - accuracy: 0.9979 - val_loss: 1.2740 - val_accuracy: 0.8359\n",
      "Epoch 23/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0024 - accuracy: 0.9984 - val_loss: 1.2557 - val_accuracy: 0.8398\n",
      "Epoch 24/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0027 - accuracy: 0.9980 - val_loss: 1.2715 - val_accuracy: 0.8367\n",
      "Epoch 25/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0026 - accuracy: 0.9984 - val_loss: 1.2435 - val_accuracy: 0.8359\n",
      "Epoch 26/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0030 - accuracy: 0.9979 - val_loss: 1.2504 - val_accuracy: 0.8383\n",
      "Epoch 27/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0027 - accuracy: 0.9979 - val_loss: 1.2542 - val_accuracy: 0.8398\n",
      "Epoch 28/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0069 - accuracy: 0.9980 - val_loss: 1.2547 - val_accuracy: 0.8383\n",
      "Epoch 29/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0027 - accuracy: 0.9986 - val_loss: 1.2567 - val_accuracy: 0.8336\n",
      "Epoch 30/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0030 - accuracy: 0.9980 - val_loss: 1.2889 - val_accuracy: 0.8344\n",
      "Epoch 31/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0055 - accuracy: 0.9980 - val_loss: 1.2745 - val_accuracy: 0.8297\n",
      "Epoch 32/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0025 - accuracy: 0.9984 - val_loss: 1.2028 - val_accuracy: 0.8367\n",
      "Epoch 33/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0025 - accuracy: 0.9982 - val_loss: 1.2260 - val_accuracy: 0.8352\n",
      "Epoch 34/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0026 - accuracy: 0.9980 - val_loss: 1.2137 - val_accuracy: 0.8344\n",
      "Epoch 35/40\n",
      "80/80 [==============================] - 25s 318ms/step - loss: 0.0026 - accuracy: 0.9979 - val_loss: 1.2282 - val_accuracy: 0.8359\n",
      "Epoch 36/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0026 - accuracy: 0.9977 - val_loss: 1.2186 - val_accuracy: 0.8391\n",
      "Epoch 37/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0026 - accuracy: 0.9973 - val_loss: 1.2278 - val_accuracy: 0.8367\n",
      "Epoch 38/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0025 - accuracy: 0.9975 - val_loss: 1.2239 - val_accuracy: 0.8414\n",
      "Epoch 39/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0025 - accuracy: 0.9980 - val_loss: 1.2327 - val_accuracy: 0.8375\n",
      "Epoch 40/40\n",
      "80/80 [==============================] - 25s 317ms/step - loss: 0.0027 - accuracy: 0.9984 - val_loss: 1.2154 - val_accuracy: 0.8414\n"
     ]
    }
   ],
   "source": [
    "history = head_model.fit(train_imgs, epochs=40, validation_data=(val_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = head_model.evaluate(test_imgs, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#head_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
